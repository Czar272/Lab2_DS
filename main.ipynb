{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58546cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44e6be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/importacion.csv\", parse_dates=[\"Fecha\"])\n",
    "df.set_index(\"Fecha\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e0b9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion gasolina regular\n",
    "regular_series = df[\"Gasolina regular\"]\n",
    "\n",
    "regular_split = int(len(regular_series) * 0.7)\n",
    "regular_train = regular_series.iloc[:regular_split] \n",
    "regular_test = regular_series.iloc[regular_split:]\n",
    "\n",
    "regular_scaler = MinMaxScaler()\n",
    "regular_train_scaled = regular_scaler.fit_transform(regular_train.values.reshape(-1, 1))\n",
    "regular_test_scaled = regular_scaler.transform(regular_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f87c2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion gasolina superior\n",
    "super_series = df[\"Gasolina superior\"]\n",
    "\n",
    "super_split = int(len(regular_series) * 0.7)\n",
    "super_train = super_series.iloc[:super_split] \n",
    "super_test = super_series.iloc[super_split:]\n",
    "\n",
    "super_scaler = MinMaxScaler()\n",
    "super_train_scaled = super_scaler.fit_transform(super_train.values.reshape(-1, 1))\n",
    "super_test_scaled = super_scaler.transform(super_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "514a8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion gasolina diesel\n",
    "diesel_series = df[\"Diesel alto azufre\"]\n",
    "\n",
    "diesel_split = int(len(diesel_series) * 0.7)\n",
    "diesel_train = diesel_series.iloc[:diesel_split]\n",
    "diesel_test = diesel_series.iloc[diesel_split:]\n",
    "\n",
    "diesel_scaler = MinMaxScaler()\n",
    "diesel_train_scaled = diesel_scaler.fit_transform(diesel_train.values.reshape(-1, 1))\n",
    "diesel_test_scaled = diesel_scaler.transform(diesel_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7a5dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window=12):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data)):\n",
    "        X.append(data[i-window:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8f944e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Gasolina regular\n",
    "window_size = 12\n",
    "X_reg_train, y_reg_train = create_sequences(regular_train_scaled, window=window_size)\n",
    "X_reg_test,  y_reg_test  = create_sequences(regular_test_scaled,  window=window_size)\n",
    "\n",
    "# Para Gasolina superior\n",
    "X_sup_train, y_sup_train = create_sequences(super_train_scaled, window=window_size)\n",
    "X_sup_test,  y_sup_test  = create_sequences(super_test_scaled,  window=window_size)\n",
    "\n",
    "# Para Gasolina diesel\n",
    "x_dis_train, y_dis_train = create_sequences(diesel_train_scaled, window=window_size)\n",
    "x_dis_test, y_dis_test = create_sequences(diesel_test_scaled, window=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0972a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimensiona al formato [muestras, timesteps, features]\n",
    "# Regular\n",
    "X_reg_train = X_reg_train.reshape(-1, window_size, 1)\n",
    "X_reg_test  = X_reg_test.reshape(-1,  window_size, 1)\n",
    "\n",
    "# Superior\n",
    "X_sup_train = X_sup_train.reshape(-1, window_size, 1)\n",
    "X_sup_test  = X_sup_test.reshape(-1,  window_size, 1)\n",
    "\n",
    "# Diesel\n",
    "x_dis_train = x_dis_train.reshape(-1, window_size, 1)\n",
    "x_dis_test = x_dis_test.reshape(-1, window_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d1513ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte NumPy → Torch Tensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Gasolina regular\n",
    "X_reg_train_t = torch.tensor(X_reg_train, dtype=torch.float32).to(device)\n",
    "y_reg_train_t = torch.tensor(y_reg_train, dtype=torch.float32).to(device)\n",
    "X_reg_test_t  = torch.tensor(X_reg_test,  dtype=torch.float32).to(device)\n",
    "y_reg_test_t  = torch.tensor(y_reg_test,  dtype=torch.float32).to(device)\n",
    "\n",
    "# Gasolina superior\n",
    "X_sup_train_t = torch.tensor(X_sup_train, dtype=torch.float32).to(device)\n",
    "y_sup_train_t = torch.tensor(y_sup_train, dtype=torch.float32).to(device)\n",
    "X_sup_test_t  = torch.tensor(X_sup_test,  dtype=torch.float32).to(device)\n",
    "y_sup_test_t  = torch.tensor(y_sup_test,  dtype=torch.float32).to(device)\n",
    "\n",
    "# Gasolina diesel\n",
    "x_dis_train_t = torch.tensor(x_dis_train, dtype=torch.float32).to(device)\n",
    "y_dis_train_t = torch.tensor(y_dis_train, dtype=torch.float32).to(device)\n",
    "x_dis_test_t = torch.tensor(x_dis_test, dtype=torch.float32).to(device)\n",
    "y_dis_test_t = torch.tensor(y_dis_test, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eaf772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset y DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# Gasolina regular\n",
    "train_ds = TensorDataset(X_reg_train_t, y_reg_train_t)\n",
    "test_ds  = TensorDataset(X_reg_test_t,  y_reg_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "# Gasolina superior\n",
    "train_ds_sup = TensorDataset(X_sup_train_t, y_sup_train_t)\n",
    "test_ds_sup  = TensorDataset(X_sup_test_t,  y_sup_test_t)\n",
    "\n",
    "train_loader_sup = DataLoader(train_ds_sup, batch_size=64, shuffle=True)\n",
    "test_loader_sup  = DataLoader(test_ds_sup,  batch_size=64)\n",
    "\n",
    "# Gasolina diesel\n",
    "train_ds_dis = TensorDataset(x_dis_train_t, y_dis_train_t)\n",
    "test_ds_dis = TensorDataset(x_dis_test_t, y_dis_test_t)\n",
    "\n",
    "train_loader_dis = DataLoader(train_ds_dis, batch_size=64, shuffle=True)\n",
    "test_loader_dis = DataLoader(test_ds_dis, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3537ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dos arquitecturas de LSTM\n",
    "class LSTMModelA(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=50, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        y = self.dropout(last)\n",
    "        return self.fc(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dfa2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelB(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_sizes=(100,50)):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size,  hidden_sizes[0], batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)\n",
    "        self.fc     = nn.Linear(hidden_sizes[1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out     = self.dropout1(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        last    = out[:, -1, :]\n",
    "        return self.fc(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf4a301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle de entrenamiento y validación\n",
    "def train_model(model, train_loader, test_loader, epochs=50, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_loss = float('inf')\n",
    "    patience, counter = 5, 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb).squeeze()\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item() * xb.size(0)\n",
    "        train_loss = running / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                preds = model(xb).squeeze()\n",
    "                running += criterion(preds, yb).item() * xb.size(0)\n",
    "        val_loss = running / len(test_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch} — Train MSE: {train_loss:.4f} — Val MSE: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_modelA.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"best_modelA.pth\"))\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a69c3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_size': [50, 100],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "}\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "039c9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando hs=50, dr=0.2, lr=0.001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.1633 — Val MSE: 1.1606\n",
      "Epoch 2 — Train MSE: 0.1167 — Val MSE: 0.9447\n",
      "Epoch 3 — Train MSE: 0.0741 — Val MSE: 0.6761\n",
      "Epoch 4 — Train MSE: 0.0527 — Val MSE: 0.4197\n",
      "Epoch 5 — Train MSE: 0.0575 — Val MSE: 0.4568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([12, 1])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train MSE: 0.0511 — Val MSE: 0.6214\n",
      "Epoch 7 — Train MSE: 0.0533 — Val MSE: 0.6681\n",
      "Epoch 8 — Train MSE: 0.0542 — Val MSE: 0.6310\n",
      "Epoch 9 — Train MSE: 0.0510 — Val MSE: 0.5377\n",
      "Early stopping.\n",
      "Probando hs=50, dr=0.2, lr=0.001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.1708 — Val MSE: 1.1474\n",
      "Epoch 2 — Train MSE: 0.1378 — Val MSE: 1.0041\n",
      "Epoch 3 — Train MSE: 0.1053 — Val MSE: 0.8574\n",
      "Epoch 4 — Train MSE: 0.0777 — Val MSE: 0.7009\n",
      "Epoch 5 — Train MSE: 0.0601 — Val MSE: 0.5539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 — Train MSE: 0.0525 — Val MSE: 0.4404\n",
      "Epoch 7 — Train MSE: 0.0534 — Val MSE: 0.3808\n",
      "Epoch 8 — Train MSE: 0.0555 — Val MSE: 0.3969\n",
      "Epoch 9 — Train MSE: 0.0547 — Val MSE: 0.4390\n",
      "Epoch 10 — Train MSE: 0.0516 — Val MSE: 0.4962\n",
      "Epoch 11 — Train MSE: 0.0505 — Val MSE: 0.5005\n",
      "Epoch 12 — Train MSE: 0.0503 — Val MSE: 0.5217\n",
      "Early stopping.\n",
      "Probando hs=50, dr=0.2, lr=0.0001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.1087 — Val MSE: 1.1120\n",
      "Epoch 2 — Train MSE: 0.1059 — Val MSE: 1.0865\n",
      "Epoch 3 — Train MSE: 0.1003 — Val MSE: 1.0617\n",
      "Epoch 4 — Train MSE: 0.0958 — Val MSE: 1.0383\n",
      "Epoch 5 — Train MSE: 0.0912 — Val MSE: 1.0171\n",
      "Epoch 6 — Train MSE: 0.0878 — Val MSE: 0.9966\n",
      "Epoch 7 — Train MSE: 0.0849 — Val MSE: 0.9755\n",
      "Epoch 8 — Train MSE: 0.0814 — Val MSE: 0.9558\n",
      "Epoch 9 — Train MSE: 0.0782 — Val MSE: 0.9348\n",
      "Epoch 10 — Train MSE: 0.0756 — Val MSE: 0.9149\n",
      "Epoch 11 — Train MSE: 0.0728 — Val MSE: 0.8964\n",
      "Epoch 12 — Train MSE: 0.0710 — Val MSE: 0.8774\n",
      "Epoch 13 — Train MSE: 0.0676 — Val MSE: 0.8583\n",
      "Epoch 14 — Train MSE: 0.0650 — Val MSE: 0.8380\n",
      "Epoch 15 — Train MSE: 0.0624 — Val MSE: 0.8150\n",
      "Epoch 16 — Train MSE: 0.0604 — Val MSE: 0.7909\n",
      "Epoch 17 — Train MSE: 0.0581 — Val MSE: 0.7714\n",
      "Epoch 18 — Train MSE: 0.0564 — Val MSE: 0.7515\n",
      "Epoch 19 — Train MSE: 0.0549 — Val MSE: 0.7321\n",
      "Epoch 20 — Train MSE: 0.0537 — Val MSE: 0.7152\n",
      "Epoch 21 — Train MSE: 0.0528 — Val MSE: 0.6962\n",
      "Epoch 22 — Train MSE: 0.0515 — Val MSE: 0.6838\n",
      "Epoch 23 — Train MSE: 0.0512 — Val MSE: 0.6740\n",
      "Epoch 24 — Train MSE: 0.0513 — Val MSE: 0.6607\n",
      "Epoch 25 — Train MSE: 0.0502 — Val MSE: 0.6507\n",
      "Epoch 26 — Train MSE: 0.0500 — Val MSE: 0.6427\n",
      "Epoch 27 — Train MSE: 0.0495 — Val MSE: 0.6400\n",
      "Epoch 28 — Train MSE: 0.0496 — Val MSE: 0.6381\n",
      "Epoch 29 — Train MSE: 0.0500 — Val MSE: 0.6411\n",
      "Epoch 30 — Train MSE: 0.0499 — Val MSE: 0.6349\n",
      "Probando hs=50, dr=0.2, lr=0.0001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.1681 — Val MSE: 1.2633\n",
      "Epoch 2 — Train MSE: 0.1647 — Val MSE: 1.2506\n",
      "Epoch 3 — Train MSE: 0.1601 — Val MSE: 1.2378\n",
      "Epoch 4 — Train MSE: 0.1584 — Val MSE: 1.2248\n",
      "Epoch 5 — Train MSE: 0.1558 — Val MSE: 1.2119\n",
      "Epoch 6 — Train MSE: 0.1509 — Val MSE: 1.1992\n",
      "Epoch 7 — Train MSE: 0.1468 — Val MSE: 1.1864\n",
      "Epoch 8 — Train MSE: 0.1462 — Val MSE: 1.1740\n",
      "Epoch 9 — Train MSE: 0.1429 — Val MSE: 1.1608\n",
      "Epoch 10 — Train MSE: 0.1390 — Val MSE: 1.1478\n",
      "Epoch 11 — Train MSE: 0.1360 — Val MSE: 1.1354\n",
      "Epoch 12 — Train MSE: 0.1337 — Val MSE: 1.1230\n",
      "Epoch 13 — Train MSE: 0.1309 — Val MSE: 1.1114\n",
      "Epoch 14 — Train MSE: 0.1275 — Val MSE: 1.0993\n",
      "Epoch 15 — Train MSE: 0.1256 — Val MSE: 1.0863\n",
      "Epoch 16 — Train MSE: 0.1207 — Val MSE: 1.0735\n",
      "Epoch 17 — Train MSE: 0.1185 — Val MSE: 1.0612\n",
      "Epoch 18 — Train MSE: 0.1157 — Val MSE: 1.0490\n",
      "Epoch 19 — Train MSE: 0.1140 — Val MSE: 1.0360\n",
      "Epoch 20 — Train MSE: 0.1098 — Val MSE: 1.0205\n",
      "Epoch 21 — Train MSE: 0.1065 — Val MSE: 1.0051\n",
      "Epoch 22 — Train MSE: 0.1039 — Val MSE: 0.9901\n",
      "Epoch 23 — Train MSE: 0.1000 — Val MSE: 0.9732\n",
      "Epoch 24 — Train MSE: 0.0969 — Val MSE: 0.9566\n",
      "Epoch 25 — Train MSE: 0.0948 — Val MSE: 0.9405\n",
      "Epoch 26 — Train MSE: 0.0894 — Val MSE: 0.9238\n",
      "Epoch 27 — Train MSE: 0.0883 — Val MSE: 0.9073\n",
      "Epoch 28 — Train MSE: 0.0859 — Val MSE: 0.8904\n",
      "Epoch 29 — Train MSE: 0.0827 — Val MSE: 0.8744\n",
      "Epoch 30 — Train MSE: 0.0789 — Val MSE: 0.8582\n",
      "Probando hs=50, dr=0.3, lr=0.001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.2533 — Val MSE: 1.3796\n",
      "Epoch 2 — Train MSE: 0.1810 — Val MSE: 1.0898\n",
      "Epoch 3 — Train MSE: 0.1036 — Val MSE: 0.6767\n",
      "Epoch 4 — Train MSE: 0.0578 — Val MSE: 0.3246\n",
      "Epoch 5 — Train MSE: 0.0597 — Val MSE: 0.4936\n",
      "Epoch 6 — Train MSE: 0.0561 — Val MSE: 0.6641\n",
      "Epoch 7 — Train MSE: 0.0612 — Val MSE: 0.6199\n",
      "Epoch 8 — Train MSE: 0.0553 — Val MSE: 0.4487\n",
      "Epoch 9 — Train MSE: 0.0551 — Val MSE: 0.3984\n",
      "Early stopping.\n",
      "Probando hs=50, dr=0.3, lr=0.001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.0804 — Val MSE: 0.8502\n",
      "Epoch 2 — Train MSE: 0.0648 — Val MSE: 0.7611\n",
      "Epoch 3 — Train MSE: 0.0572 — Val MSE: 0.7014\n",
      "Epoch 4 — Train MSE: 0.0543 — Val MSE: 0.6591\n",
      "Epoch 5 — Train MSE: 0.0519 — Val MSE: 0.6158\n",
      "Epoch 6 — Train MSE: 0.0507 — Val MSE: 0.5702\n",
      "Epoch 7 — Train MSE: 0.0504 — Val MSE: 0.5551\n",
      "Epoch 8 — Train MSE: 0.0501 — Val MSE: 0.5592\n",
      "Epoch 9 — Train MSE: 0.0501 — Val MSE: 0.5689\n",
      "Epoch 10 — Train MSE: 0.0506 — Val MSE: 0.5876\n",
      "Epoch 11 — Train MSE: 0.0503 — Val MSE: 0.5755\n",
      "Epoch 12 — Train MSE: 0.0502 — Val MSE: 0.5704\n",
      "Early stopping.\n",
      "Probando hs=50, dr=0.3, lr=0.0001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.1686 — Val MSE: 1.2857\n",
      "Epoch 2 — Train MSE: 0.1661 — Val MSE: 1.2647\n",
      "Epoch 3 — Train MSE: 0.1594 — Val MSE: 1.2437\n",
      "Epoch 4 — Train MSE: 0.1560 — Val MSE: 1.2232\n",
      "Epoch 5 — Train MSE: 0.1488 — Val MSE: 1.2030\n",
      "Epoch 6 — Train MSE: 0.1440 — Val MSE: 1.1831\n",
      "Epoch 7 — Train MSE: 0.1378 — Val MSE: 1.1635\n",
      "Epoch 8 — Train MSE: 0.1335 — Val MSE: 1.1433\n",
      "Epoch 9 — Train MSE: 0.1313 — Val MSE: 1.1219\n",
      "Epoch 10 — Train MSE: 0.1255 — Val MSE: 1.1000\n",
      "Epoch 11 — Train MSE: 0.1219 — Val MSE: 1.0780\n",
      "Epoch 12 — Train MSE: 0.1137 — Val MSE: 1.0548\n",
      "Epoch 13 — Train MSE: 0.1121 — Val MSE: 1.0310\n",
      "Epoch 14 — Train MSE: 0.1063 — Val MSE: 1.0063\n",
      "Epoch 15 — Train MSE: 0.1028 — Val MSE: 0.9812\n",
      "Epoch 16 — Train MSE: 0.0982 — Val MSE: 0.9558\n",
      "Epoch 17 — Train MSE: 0.0902 — Val MSE: 0.9289\n",
      "Epoch 18 — Train MSE: 0.0866 — Val MSE: 0.9003\n",
      "Epoch 19 — Train MSE: 0.0836 — Val MSE: 0.8715\n",
      "Epoch 20 — Train MSE: 0.0791 — Val MSE: 0.8374\n",
      "Epoch 21 — Train MSE: 0.0725 — Val MSE: 0.8034\n",
      "Epoch 22 — Train MSE: 0.0693 — Val MSE: 0.7744\n",
      "Epoch 23 — Train MSE: 0.0670 — Val MSE: 0.7441\n",
      "Epoch 24 — Train MSE: 0.0624 — Val MSE: 0.7193\n",
      "Epoch 25 — Train MSE: 0.0610 — Val MSE: 0.6901\n",
      "Epoch 26 — Train MSE: 0.0586 — Val MSE: 0.6567\n",
      "Epoch 27 — Train MSE: 0.0553 — Val MSE: 0.6295\n",
      "Epoch 28 — Train MSE: 0.0547 — Val MSE: 0.6067\n",
      "Epoch 29 — Train MSE: 0.0527 — Val MSE: 0.5893\n",
      "Epoch 30 — Train MSE: 0.0517 — Val MSE: 0.5740\n",
      "Probando hs=50, dr=0.3, lr=0.0001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.1610 — Val MSE: 1.2992\n",
      "Epoch 2 — Train MSE: 0.1567 — Val MSE: 1.2853\n",
      "Epoch 3 — Train MSE: 0.1539 — Val MSE: 1.2711\n",
      "Epoch 4 — Train MSE: 0.1513 — Val MSE: 1.2573\n",
      "Epoch 5 — Train MSE: 0.1459 — Val MSE: 1.2439\n",
      "Epoch 6 — Train MSE: 0.1423 — Val MSE: 1.2306\n",
      "Epoch 7 — Train MSE: 0.1402 — Val MSE: 1.2171\n",
      "Epoch 8 — Train MSE: 0.1351 — Val MSE: 1.2034\n",
      "Epoch 9 — Train MSE: 0.1337 — Val MSE: 1.1895\n",
      "Epoch 10 — Train MSE: 0.1310 — Val MSE: 1.1759\n",
      "Epoch 11 — Train MSE: 0.1271 — Val MSE: 1.1618\n",
      "Epoch 12 — Train MSE: 0.1252 — Val MSE: 1.1479\n",
      "Epoch 13 — Train MSE: 0.1216 — Val MSE: 1.1344\n",
      "Epoch 14 — Train MSE: 0.1203 — Val MSE: 1.1213\n",
      "Epoch 15 — Train MSE: 0.1127 — Val MSE: 1.1083\n",
      "Epoch 16 — Train MSE: 0.1137 — Val MSE: 1.0949\n",
      "Epoch 17 — Train MSE: 0.1109 — Val MSE: 1.0813\n",
      "Epoch 18 — Train MSE: 0.1083 — Val MSE: 1.0678\n",
      "Epoch 19 — Train MSE: 0.1068 — Val MSE: 1.0538\n",
      "Epoch 20 — Train MSE: 0.1021 — Val MSE: 1.0387\n",
      "Epoch 21 — Train MSE: 0.0985 — Val MSE: 1.0241\n",
      "Epoch 22 — Train MSE: 0.0974 — Val MSE: 1.0094\n",
      "Epoch 23 — Train MSE: 0.0936 — Val MSE: 0.9936\n",
      "Epoch 24 — Train MSE: 0.0924 — Val MSE: 0.9784\n",
      "Epoch 25 — Train MSE: 0.0901 — Val MSE: 0.9628\n",
      "Epoch 26 — Train MSE: 0.0865 — Val MSE: 0.9491\n",
      "Epoch 27 — Train MSE: 0.0843 — Val MSE: 0.9358\n",
      "Epoch 28 — Train MSE: 0.0828 — Val MSE: 0.9211\n",
      "Epoch 29 — Train MSE: 0.0783 — Val MSE: 0.9072\n",
      "Epoch 30 — Train MSE: 0.0765 — Val MSE: 0.8913\n",
      "Probando hs=100, dr=0.2, lr=0.001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.0949 — Val MSE: 0.7878\n",
      "Epoch 2 — Train MSE: 0.0543 — Val MSE: 0.5030\n",
      "Epoch 3 — Train MSE: 0.0559 — Val MSE: 0.4986\n",
      "Epoch 4 — Train MSE: 0.0508 — Val MSE: 0.6345\n",
      "Epoch 5 — Train MSE: 0.0510 — Val MSE: 0.6724\n",
      "Epoch 6 — Train MSE: 0.0511 — Val MSE: 0.6293\n",
      "Epoch 7 — Train MSE: 0.0493 — Val MSE: 0.5862\n",
      "Epoch 8 — Train MSE: 0.0491 — Val MSE: 0.5845\n",
      "Early stopping.\n",
      "Probando hs=100, dr=0.2, lr=0.001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.2295 — Val MSE: 1.3375\n",
      "Epoch 2 — Train MSE: 0.1653 — Val MSE: 1.0797\n",
      "Epoch 3 — Train MSE: 0.1046 — Val MSE: 0.7390\n",
      "Epoch 4 — Train MSE: 0.0532 — Val MSE: 0.3239\n",
      "Epoch 5 — Train MSE: 0.0783 — Val MSE: 0.2129\n",
      "Epoch 6 — Train MSE: 0.0849 — Val MSE: 0.3685\n",
      "Epoch 7 — Train MSE: 0.0526 — Val MSE: 0.5821\n",
      "Epoch 8 — Train MSE: 0.0540 — Val MSE: 0.6881\n",
      "Epoch 9 — Train MSE: 0.0620 — Val MSE: 0.7097\n",
      "Epoch 10 — Train MSE: 0.0611 — Val MSE: 0.6515\n",
      "Early stopping.\n",
      "Probando hs=100, dr=0.2, lr=0.0001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.2704 — Val MSE: 1.5991\n",
      "Epoch 2 — Train MSE: 0.2546 — Val MSE: 1.5533\n",
      "Epoch 3 — Train MSE: 0.2411 — Val MSE: 1.5089\n",
      "Epoch 4 — Train MSE: 0.2296 — Val MSE: 1.4653\n",
      "Epoch 5 — Train MSE: 0.2155 — Val MSE: 1.4220\n",
      "Epoch 6 — Train MSE: 0.2043 — Val MSE: 1.3775\n",
      "Epoch 7 — Train MSE: 0.1920 — Val MSE: 1.3303\n",
      "Epoch 8 — Train MSE: 0.1806 — Val MSE: 1.2809\n",
      "Epoch 9 — Train MSE: 0.1673 — Val MSE: 1.2258\n",
      "Epoch 10 — Train MSE: 0.1535 — Val MSE: 1.1687\n",
      "Epoch 11 — Train MSE: 0.1400 — Val MSE: 1.1034\n",
      "Epoch 12 — Train MSE: 0.1244 — Val MSE: 1.0328\n",
      "Epoch 13 — Train MSE: 0.1106 — Val MSE: 0.9526\n",
      "Epoch 14 — Train MSE: 0.0957 — Val MSE: 0.8716\n",
      "Epoch 15 — Train MSE: 0.0830 — Val MSE: 0.7875\n",
      "Epoch 16 — Train MSE: 0.0714 — Val MSE: 0.6996\n",
      "Epoch 17 — Train MSE: 0.0619 — Val MSE: 0.6142\n",
      "Epoch 18 — Train MSE: 0.0546 — Val MSE: 0.5446\n",
      "Epoch 19 — Train MSE: 0.0512 — Val MSE: 0.4856\n",
      "Epoch 20 — Train MSE: 0.0500 — Val MSE: 0.4398\n",
      "Epoch 21 — Train MSE: 0.0500 — Val MSE: 0.4199\n",
      "Epoch 22 — Train MSE: 0.0510 — Val MSE: 0.3910\n",
      "Epoch 23 — Train MSE: 0.0518 — Val MSE: 0.3846\n",
      "Epoch 24 — Train MSE: 0.0530 — Val MSE: 0.3843\n",
      "Epoch 25 — Train MSE: 0.0517 — Val MSE: 0.4326\n",
      "Epoch 26 — Train MSE: 0.0501 — Val MSE: 0.4773\n",
      "Epoch 27 — Train MSE: 0.0505 — Val MSE: 0.4904\n",
      "Epoch 28 — Train MSE: 0.0505 — Val MSE: 0.4912\n",
      "Epoch 29 — Train MSE: 0.0501 — Val MSE: 0.4728\n",
      "Early stopping.\n",
      "Probando hs=100, dr=0.2, lr=0.0001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.2023 — Val MSE: 1.3878\n",
      "Epoch 2 — Train MSE: 0.1963 — Val MSE: 1.3665\n",
      "Epoch 3 — Train MSE: 0.1905 — Val MSE: 1.3453\n",
      "Epoch 4 — Train MSE: 0.1855 — Val MSE: 1.3240\n",
      "Epoch 5 — Train MSE: 0.1787 — Val MSE: 1.3028\n",
      "Epoch 6 — Train MSE: 0.1736 — Val MSE: 1.2816\n",
      "Epoch 7 — Train MSE: 0.1683 — Val MSE: 1.2602\n",
      "Epoch 8 — Train MSE: 0.1629 — Val MSE: 1.2381\n",
      "Epoch 9 — Train MSE: 0.1574 — Val MSE: 1.2158\n",
      "Epoch 10 — Train MSE: 0.1511 — Val MSE: 1.1940\n",
      "Epoch 11 — Train MSE: 0.1465 — Val MSE: 1.1710\n",
      "Epoch 12 — Train MSE: 0.1413 — Val MSE: 1.1470\n",
      "Epoch 13 — Train MSE: 0.1360 — Val MSE: 1.1218\n",
      "Epoch 14 — Train MSE: 0.1294 — Val MSE: 1.0953\n",
      "Epoch 15 — Train MSE: 0.1243 — Val MSE: 1.0664\n",
      "Epoch 16 — Train MSE: 0.1172 — Val MSE: 1.0353\n",
      "Epoch 17 — Train MSE: 0.1111 — Val MSE: 1.0006\n",
      "Epoch 18 — Train MSE: 0.1050 — Val MSE: 0.9629\n",
      "Epoch 19 — Train MSE: 0.0973 — Val MSE: 0.9242\n",
      "Epoch 20 — Train MSE: 0.0896 — Val MSE: 0.8836\n",
      "Epoch 21 — Train MSE: 0.0831 — Val MSE: 0.8379\n",
      "Epoch 22 — Train MSE: 0.0755 — Val MSE: 0.7922\n",
      "Epoch 23 — Train MSE: 0.0694 — Val MSE: 0.7431\n",
      "Epoch 24 — Train MSE: 0.0637 — Val MSE: 0.6930\n",
      "Epoch 25 — Train MSE: 0.0595 — Val MSE: 0.6514\n",
      "Epoch 26 — Train MSE: 0.0547 — Val MSE: 0.6098\n",
      "Epoch 27 — Train MSE: 0.0525 — Val MSE: 0.5641\n",
      "Epoch 28 — Train MSE: 0.0508 — Val MSE: 0.5291\n",
      "Epoch 29 — Train MSE: 0.0498 — Val MSE: 0.5154\n",
      "Epoch 30 — Train MSE: 0.0498 — Val MSE: 0.5108\n",
      "Probando hs=100, dr=0.3, lr=0.001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.0928 — Val MSE: 0.7195\n",
      "Epoch 2 — Train MSE: 0.0541 — Val MSE: 0.4132\n",
      "Epoch 3 — Train MSE: 0.0575 — Val MSE: 0.4269\n",
      "Epoch 4 — Train MSE: 0.0500 — Val MSE: 0.5875\n",
      "Epoch 5 — Train MSE: 0.0516 — Val MSE: 0.6011\n",
      "Epoch 6 — Train MSE: 0.0509 — Val MSE: 0.5297\n",
      "Epoch 7 — Train MSE: 0.0496 — Val MSE: 0.5219\n",
      "Early stopping.\n",
      "Probando hs=100, dr=0.3, lr=0.001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.1446 — Val MSE: 1.0906\n",
      "Epoch 2 — Train MSE: 0.1053 — Val MSE: 0.9013\n",
      "Epoch 3 — Train MSE: 0.0728 — Val MSE: 0.6717\n",
      "Epoch 4 — Train MSE: 0.0517 — Val MSE: 0.4203\n",
      "Epoch 5 — Train MSE: 0.0546 — Val MSE: 0.3821\n",
      "Epoch 6 — Train MSE: 0.0614 — Val MSE: 0.3568\n",
      "Epoch 7 — Train MSE: 0.0582 — Val MSE: 0.4663\n",
      "Epoch 8 — Train MSE: 0.0497 — Val MSE: 0.5760\n",
      "Epoch 9 — Train MSE: 0.0517 — Val MSE: 0.6419\n",
      "Epoch 10 — Train MSE: 0.0540 — Val MSE: 0.6636\n",
      "Epoch 11 — Train MSE: 0.0546 — Val MSE: 0.6448\n",
      "Early stopping.\n",
      "Probando hs=100, dr=0.3, lr=0.0001, bs=32…\n",
      "Epoch 1 — Train MSE: 0.1031 — Val MSE: 0.9906\n",
      "Epoch 2 — Train MSE: 0.0977 — Val MSE: 0.9636\n",
      "Epoch 3 — Train MSE: 0.0916 — Val MSE: 0.9379\n",
      "Epoch 4 — Train MSE: 0.0871 — Val MSE: 0.9115\n",
      "Epoch 5 — Train MSE: 0.0804 — Val MSE: 0.8845\n",
      "Epoch 6 — Train MSE: 0.0762 — Val MSE: 0.8567\n",
      "Epoch 7 — Train MSE: 0.0722 — Val MSE: 0.8287\n",
      "Epoch 8 — Train MSE: 0.0684 — Val MSE: 0.8009\n",
      "Epoch 9 — Train MSE: 0.0644 — Val MSE: 0.7741\n",
      "Epoch 10 — Train MSE: 0.0611 — Val MSE: 0.7445\n",
      "Epoch 11 — Train MSE: 0.0576 — Val MSE: 0.7144\n",
      "Epoch 12 — Train MSE: 0.0559 — Val MSE: 0.6899\n",
      "Epoch 13 — Train MSE: 0.0535 — Val MSE: 0.6684\n",
      "Epoch 14 — Train MSE: 0.0521 — Val MSE: 0.6466\n",
      "Epoch 15 — Train MSE: 0.0503 — Val MSE: 0.6230\n",
      "Epoch 16 — Train MSE: 0.0500 — Val MSE: 0.6029\n",
      "Epoch 17 — Train MSE: 0.0495 — Val MSE: 0.5875\n",
      "Epoch 18 — Train MSE: 0.0494 — Val MSE: 0.5852\n",
      "Epoch 19 — Train MSE: 0.0495 — Val MSE: 0.5887\n",
      "Epoch 20 — Train MSE: 0.0492 — Val MSE: 0.5863\n",
      "Epoch 21 — Train MSE: 0.0495 — Val MSE: 0.5840\n",
      "Epoch 22 — Train MSE: 0.0494 — Val MSE: 0.5895\n",
      "Epoch 23 — Train MSE: 0.0494 — Val MSE: 0.5916\n",
      "Epoch 24 — Train MSE: 0.0493 — Val MSE: 0.5732\n",
      "Epoch 25 — Train MSE: 0.0492 — Val MSE: 0.5503\n",
      "Epoch 26 — Train MSE: 0.0494 — Val MSE: 0.5479\n",
      "Epoch 27 — Train MSE: 0.0492 — Val MSE: 0.5588\n",
      "Epoch 28 — Train MSE: 0.0490 — Val MSE: 0.5726\n",
      "Epoch 29 — Train MSE: 0.0493 — Val MSE: 0.5787\n",
      "Epoch 30 — Train MSE: 0.0495 — Val MSE: 0.5858\n",
      "Probando hs=100, dr=0.3, lr=0.0001, bs=64…\n",
      "Epoch 1 — Train MSE: 0.1911 — Val MSE: 1.4051\n",
      "Epoch 2 — Train MSE: 0.1850 — Val MSE: 1.3827\n",
      "Epoch 3 — Train MSE: 0.1772 — Val MSE: 1.3611\n",
      "Epoch 4 — Train MSE: 0.1732 — Val MSE: 1.3401\n",
      "Epoch 5 — Train MSE: 0.1659 — Val MSE: 1.3192\n",
      "Epoch 6 — Train MSE: 0.1627 — Val MSE: 1.2990\n",
      "Epoch 7 — Train MSE: 0.1584 — Val MSE: 1.2799\n",
      "Epoch 8 — Train MSE: 0.1536 — Val MSE: 1.2611\n",
      "Epoch 9 — Train MSE: 0.1480 — Val MSE: 1.2422\n",
      "Epoch 10 — Train MSE: 0.1428 — Val MSE: 1.2236\n",
      "Epoch 11 — Train MSE: 0.1386 — Val MSE: 1.2049\n",
      "Epoch 12 — Train MSE: 0.1338 — Val MSE: 1.1865\n",
      "Epoch 13 — Train MSE: 0.1280 — Val MSE: 1.1674\n",
      "Epoch 14 — Train MSE: 0.1252 — Val MSE: 1.1473\n",
      "Epoch 15 — Train MSE: 0.1200 — Val MSE: 1.1264\n",
      "Epoch 16 — Train MSE: 0.1157 — Val MSE: 1.1044\n",
      "Epoch 17 — Train MSE: 0.1108 — Val MSE: 1.0820\n",
      "Epoch 18 — Train MSE: 0.1064 — Val MSE: 1.0596\n",
      "Epoch 19 — Train MSE: 0.1018 — Val MSE: 1.0367\n",
      "Epoch 20 — Train MSE: 0.0969 — Val MSE: 1.0131\n",
      "Epoch 21 — Train MSE: 0.0916 — Val MSE: 0.9876\n",
      "Epoch 22 — Train MSE: 0.0884 — Val MSE: 0.9593\n",
      "Epoch 23 — Train MSE: 0.0832 — Val MSE: 0.9306\n",
      "Epoch 24 — Train MSE: 0.0782 — Val MSE: 0.9022\n",
      "Epoch 25 — Train MSE: 0.0751 — Val MSE: 0.8736\n",
      "Epoch 26 — Train MSE: 0.0700 — Val MSE: 0.8444\n",
      "Epoch 27 — Train MSE: 0.0663 — Val MSE: 0.8111\n",
      "Epoch 28 — Train MSE: 0.0631 — Val MSE: 0.7793\n",
      "Epoch 29 — Train MSE: 0.0592 — Val MSE: 0.7420\n",
      "Epoch 30 — Train MSE: 0.0563 — Val MSE: 0.7124\n"
     ]
    }
   ],
   "source": [
    "for hs, dr, lr, bs in itertools.product(\n",
    "        param_grid['hidden_size'],\n",
    "        param_grid['dropout_rate'],\n",
    "        param_grid['lr'],\n",
    "        param_grid['batch_size']\n",
    "    ):\n",
    "    print(f\"Probando hs={hs}, dr={dr}, lr={lr}, bs={bs}…\")\n",
    "\n",
    "    # Redefine modelo con estos hiperparámetros\n",
    "    model = LSTMModelA(input_size=1, hidden_size=hs, dropout_rate=dr)\n",
    "\n",
    "    # DataLoaders con nuevo batch_size\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=bs)\n",
    "    \n",
    "    # Entrena y captura el val_loss\n",
    "    _, val_loss = train_model(model, train_loader, test_loader, epochs=30, lr=lr)\n",
    "    results.append({\n",
    "        'hidden_size': hs,\n",
    "        'dropout_rate': dr,\n",
    "        'lr': lr,\n",
    "        'batch_size': bs,\n",
    "        'val_loss': val_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ae0fdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hidden_size  dropout_rate      lr  batch_size  val_loss\n",
      "9           100           0.2  0.0010          64  0.212905\n",
      "4            50           0.3  0.0010          32  0.324617\n",
      "13          100           0.3  0.0010          64  0.356820\n",
      "1            50           0.2  0.0010          64  0.380771\n",
      "10          100           0.2  0.0001          32  0.384292\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.sort_values('val_loss').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c56454ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores hiperparametros\n",
    "best_params_reg = {\n",
    "    'hidden_size': 100,\n",
    "    'dropout_rate': 0.3,\n",
    "    'lr': 1e-3,\n",
    "    'batch_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ffb67",
   "metadata": {},
   "source": [
    "### Mejor modelo para gasolina regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "595d13ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train MSE: 0.1642 — Val MSE: 1.1269\n",
      "Epoch 2 — Train MSE: 0.1171 — Val MSE: 0.9335\n",
      "Epoch 3 — Train MSE: 0.0806 — Val MSE: 0.7179\n",
      "Epoch 4 — Train MSE: 0.0559 — Val MSE: 0.4862\n",
      "Epoch 5 — Train MSE: 0.0531 — Val MSE: 0.3736\n",
      "Epoch 6 — Train MSE: 0.0584 — Val MSE: 0.4330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([12, 1])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 — Train MSE: 0.0514 — Val MSE: 0.5512\n",
      "Epoch 8 — Train MSE: 0.0504 — Val MSE: 0.6294\n",
      "Epoch 9 — Train MSE: 0.0538 — Val MSE: 0.6631\n",
      "Epoch 10 — Train MSE: 0.0555 — Val MSE: 0.6551\n",
      "Early stopping.\n",
      "Mejor Val MSE (regular): 0.3736\n"
     ]
    }
   ],
   "source": [
    "# Definir modelo con estos hieperparametros\n",
    "model_reg = LSTMModelA(\n",
    "    input_size=1, \n",
    "    hidden_size=best_params_reg['hidden_size'],\n",
    "    dropout_rate=best_params_reg['dropout_rate']\n",
    ")\n",
    "\n",
    "train_loader_reg = DataLoader(train_ds, batch_size=best_params_reg['batch_size'], shuffle=True)\n",
    "test_loader_reg  = DataLoader(test_ds,  batch_size=best_params_reg['batch_size'])\n",
    "\n",
    "\n",
    "# Entrena y captura la mejor pérdida de validación\n",
    "model_reg, best_loss_reg = train_model(\n",
    "    model_reg,\n",
    "    train_loader_reg,\n",
    "    test_loader_reg,\n",
    "    epochs=50,\n",
    "    lr=best_params_reg['lr']\n",
    ")\n",
    "print(f\"Mejor Val MSE (regular): {best_loss_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8896070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gasolina Regular: \n",
      "\n",
      "MAE: 342398.5285 \n",
      "RMSE: 364936.5787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predicciones\n",
    "model_reg.eval()\n",
    "preds_reg = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader_reg:\n",
    "        preds_reg.extend(model_reg(xb).squeeze().cpu().numpy())\n",
    "preds_reg = np.array(preds_reg).reshape(-1,1)\n",
    "\n",
    "# Inversión de escala\n",
    "preds_reg_inv = regular_scaler.inverse_transform(preds_reg)\n",
    "y_reg_true    = regular_test.values[window_size:]\n",
    "\n",
    "# Metricas\n",
    "mae_reg  = mean_absolute_error(y_reg_true, preds_reg_inv)\n",
    "rmse_reg = np.sqrt(mean_squared_error(y_reg_true, preds_reg_inv))\n",
    "print(f\"Gasolina Regular: \\n\\nMAE: {mae_reg:.4f} \\nRMSE: {rmse_reg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fa1b0",
   "metadata": {},
   "source": [
    "### Mejor modelo para gasolina Superior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f873fb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train MSE: 0.1540 — Val MSE: 0.3782\n",
      "Epoch 2 — Train MSE: 0.1023 — Val MSE: 0.2804\n",
      "Epoch 3 — Train MSE: 0.0644 — Val MSE: 0.1830\n",
      "Epoch 4 — Train MSE: 0.0456 — Val MSE: 0.1199\n",
      "Epoch 5 — Train MSE: 0.0446 — Val MSE: 0.1366\n",
      "Epoch 6 — Train MSE: 0.0439 — Val MSE: 0.1611\n",
      "Epoch 7 — Train MSE: 0.0456 — Val MSE: 0.1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([12, 1])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 — Train MSE: 0.0458 — Val MSE: 0.1675\n",
      "Epoch 9 — Train MSE: 0.0452 — Val MSE: 0.1612\n",
      "Early stopping.\n",
      "Mejor Val MSE (superior): 0.1199\n"
     ]
    }
   ],
   "source": [
    "model_sup = LSTMModelA(input_size=1, hidden_size=100, dropout_rate=0.3)\n",
    "\n",
    "model_sup, best_loss_sup = train_model(\n",
    "    model_sup,\n",
    "    train_loader_sup,\n",
    "    test_loader_sup,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    ")\n",
    "print(f\"Mejor Val MSE (superior): {best_loss_sup:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cf38835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gasolina Superior:  \n",
      "\n",
      "MAE: 179196.5457 \n",
      "RMSE: 209866.8716\n"
     ]
    }
   ],
   "source": [
    "# Predicciones\n",
    "model_sup.eval()\n",
    "preds_sup = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader_sup:\n",
    "        preds_sup.extend(model_sup(xb).squeeze().cpu().numpy())\n",
    "preds_sup = np.array(preds_sup).reshape(-1,1)\n",
    "\n",
    "# Inversión de escala\n",
    "preds_sup_inv = super_scaler.inverse_transform(preds_sup)\n",
    "y_sup_true    = super_test.values[window_size:]\n",
    "\n",
    "# Métricas\n",
    "mae_sup  = mean_absolute_error(y_sup_true, preds_sup_inv)\n",
    "rmse_sup = np.sqrt(mean_squared_error(y_sup_true, preds_sup_inv))\n",
    "print(f\"Gasolina Superior:  \\n\\nMAE: {mae_sup:.4f} \\nRMSE: {rmse_sup:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6ebcd",
   "metadata": {},
   "source": [
    "### Mejor modelo para Diesel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31939ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Train MSE: 0.2631 — Val MSE: 0.0033\n",
      "Epoch 2 — Train MSE: 0.1783 — Val MSE: 0.0208\n",
      "Epoch 3 — Train MSE: 0.1031 — Val MSE: 0.0745\n",
      "Epoch 4 — Train MSE: 0.0342 — Val MSE: 0.2312\n",
      "Epoch 5 — Train MSE: 0.0443 — Val MSE: 0.2057\n",
      "Epoch 6 — Train MSE: 0.0263 — Val MSE: 0.1285\n",
      "Early stopping.\n",
      "Mejor Val MSE (diesel): 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\MSI\\Desktop\\info\\Escritorio\\UVG\\8vo Semestre\\Data Science\\DSvenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([12, 1])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model_dis = LSTMModelA(input_size=1, hidden_size=100, dropout_rate=0.3)\n",
    "\n",
    "model_dis, best_loss_dis = train_model(\n",
    "    model_dis,\n",
    "    train_loader_dis,\n",
    "    test_loader_dis,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    ")\n",
    "print(f\"Mejor Val MSE (diesel): {best_loss_dis:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cd80e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gasolina Diesel: \n",
      "\n",
      "Mae: 8460750124.5191 \n",
      "RMSE: 91982.3359\n"
     ]
    }
   ],
   "source": [
    "# Predicciones\n",
    "model_dis.eval()\n",
    "preds_dis = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader_dis:\n",
    "        preds_dis.extend(model_dis(xb).squeeze().cpu().numpy())\n",
    "preds_dis = np.array(preds_dis).reshape(-1, 1)\n",
    "\n",
    "# Inversion de escala\n",
    "preds_dis_inv = diesel_scaler.inverse_transform(preds_dis)\n",
    "y_dis_true = diesel_test.values[window_size:]\n",
    "\n",
    "# Metricas\n",
    "mae_dis = mean_squared_error(y_dis_true, preds_dis_inv)\n",
    "rmse_dis = np.sqrt(mean_squared_error(y_dis_true, preds_dis_inv))\n",
    "print(f\"Gasolina Diesel: \\n\\nMae: {mae_dis:.4f} \\nRMSE: {rmse_dis:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba3592",
   "metadata": {},
   "source": [
    "### Comparacion modelos Laboratorio 1 vs Laboratorio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2d382",
   "metadata": {},
   "source": [
    "##### __Laboratorio 1 – Modelos ARIMA__\n",
    "- Gasolina Regular\n",
    "\n",
    "    Modelo usado para predicción de los últimos 36 meses: ARIMA(45,1,20)\n",
    "\n",
    "    MAE: 102 164.24 \n",
    "\n",
    "    RMSE: 124 108.98 \n",
    "\n",
    "- Gasolina Superior\n",
    "\n",
    "    Modelo usado para predicción de los últimos 36 meses: ARIMA(45,1,20)\n",
    "    \n",
    "    MAE: 93 291.85 \n",
    "\n",
    "    RMSE: 130 298.21 \n",
    "\n",
    "- Gasolina Diesel\n",
    "\n",
    "    Sobre la ultima serie filtrada hasta diciembre 2017: ARIMA(45,1,20)\n",
    "\n",
    "    MAE: 182 448.53\n",
    "\n",
    "    RMSE: 228 476.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcc7a7",
   "metadata": {},
   "source": [
    "##### __Laboratorio 2 – Modelos LSTM__\n",
    "- Gasolina Regular\n",
    "\n",
    "    MAE: 361 909.9981\n",
    "\n",
    "    RMSE: 383 606.3430\n",
    "\n",
    "\n",
    "\n",
    "- Gasolina Superior\n",
    "\n",
    "    MAE: 116 476.7082\n",
    "\n",
    "    RMSE: 155 397.9649\n",
    "\n",
    "\n",
    "\n",
    "- Gasolina Diesel: \n",
    "\n",
    "    MAE: 8460750124.5191\n",
    "\n",
    "    RMSE: 91982.3359\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224a3a6",
   "metadata": {},
   "source": [
    "Para comparar los modelos usamos dos criterios claros sobre la serie original:\n",
    "\n",
    "* **MAE** (error absoluto medio), que nos dice cuánto, en promedio, se equivocan las predicciones.\n",
    "* **RMSE** (raíz del error cuadrático medio), que penaliza más los errores grandes.\n",
    "\n",
    "\n",
    "En **Gasolina Regular**, el LSTM obtuvo un **MAE = 361 910** y un **RMSE = 383 606**, mientras que el ARIMA(45,1,20) del laboratorio 1 logró un **MAE = 102 164** y **RMSE = 124 109** . Es decir, el ARIMA se quedó mucho más cerca de los valores reales, con errores alrededor de un tercio de los del LSTM. Por eso, para esta serie el ARIMA sigue siendo el ganador claro.\n",
    "\n",
    "En **Gasolina Superior**, el LSTM bajó el error respecto al caso anterior (MAE = 116 477, RMSE = 155 398), pero el ARIMA(45,1,20) todavía lo superó con un **MAE = 93 292** y **RMSE = 130 298** . Aunque la diferencia es menor que en la serie regular, de nuevo el ARIMA entrega predicciones más ajustadas.\n",
    "\n",
    "Para **Gasolina Diésel** vemos un comportamiento curioso: el LSTM reportó un **RMSE = 91 982**, que técnicamente es mejor que el **228 477** del ARIMA, pero su **MAE = 8 460 750 124** es colosal, lo que indica que hubo predicciones completamente fuera de escala. En cambio, el ARIMA(45,1,20) mantuvo ambos errores en rangos razonables (MAE = 182 449, RMSE = 228 477) . Esto sugiere que el LSTM, al no capturar bien la dinámica de la serie diésel, produjo picos de error muy grandes que no reflejan la realidad.\n",
    "\n",
    "**Conclusión**:\n",
    "\n",
    "* **ARIMA(45,1,20)** es el mejor modelo para las tres series cuando se mide con MAE y RMSE en la escala original.\n",
    "* El **LSTM**, aunque prometedor en otros contextos, necesita más ajustes (arquitectura, datos de entrada o exógenas) para alcanzar a los modelos clásicos en estas series de gasolina.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
